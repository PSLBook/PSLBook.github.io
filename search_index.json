[["index.html", "Practical Statistical Learning Preface", " Practical Statistical Learning John Marden and Feng Liang 2022-09-22 Preface These notes are based on a course in statistical learning developed by John Marden and Feng Liang at UIUC using the text The Elements of Statistical Learning by Hastie, Tibshirani and Friedman https://hastie.su.domains/ElemStatLearn/. "],["intro.html", "Chapter 1 Introduction 1.1 Introduction to Statistical Learning 1.2 Least squares vs. nearest neighbors", " Chapter 1 Introduction Notes [lec_W1.1_Introduction_Statistical_Learning.pdf] [lec_W1.2_kNN_vs_LinearRegression.pdf] [lec_W1.2_kNN_vs_LinearRegression_figs.pdf] [lec_W1.3_Introduction_LearningTheory.pdf] R/Python Code: [Rcode_W1_SimulationStudy.html] [Rcode_W1_Examples_from_ESL.html] [Python_W1_SimulationStudy.html] 1.1 Introduction to Statistical Learning What is machine learning? In artificial intelligence, machine learning involves some kind of machine (robot, computer) that modifies its behavior based on experience. For example, if a robot falls down every time it comes to a stairway, it will learn to avoid stairways. E-mail programs often learn to distinguish spam from regular e-mail. In statistics, machine learning uses statistical data to learn. What is data mining? Looking for relationships in large data sets. Observations are “baskets” of items. The goal is to see what items are associated with other items, or which items’ presence implies the presence of other items. For example, at Walmart, one may realize that people who buy socks also buy beer. Then Walmart would be smart to put some beer cases near the socks, or vice versa. Or if the government is spying on everyone’s e-mails, certain words (which I better not say) found together might cause the writer to be sent to Guantanamo. The difference for a statistician between supervised machine learning and regular data analysis is that in machine learning, the statistician does not care about the estimates of parameters nor hypothesis tests nor which models fit best. Rather, the focus is on finding some function that does a good job of predicting \\(y\\) from \\(x\\). Estimating parameters, fitting models, etc., may indeed be important parts of developing the function, but they are not the objective. 1.1.1 Types of learning problems Generally, there are two categories: Supervised learning data consists of example \\((y,x)\\)’s, the training data. The machine is a function built based on the data that takes in a new \\(x\\), and produces a guess of the corresponding \\(y\\). It is regression if the \\(y\\)’s are continuous, and classification or categorization if the \\(y\\)’s are categories. Unsupervised learning is clustering. The data consists of example \\(x\\)’s, and the machine is a function that groups the \\(x\\)’s into clusters. 1.1.2 Challenge of supervised learning 1.1.3 Curse of dimensionality 1.1.4 A glimpse of learning theory 1.1.5 Bias and variance tradeoff 1.2 Least squares vs. nearest neighbors 1.2.1 Introduction to LS and kNN 1.2.2 Simulation study with R 1.2.3 Simulation study with Python 1.2.4 Compute Bayes rule 1.2.5 Discussion "],["linear-model-chapter.html", "Chapter 2 Linear Regression 2.1 Good predictions: Squared error loss and in-sample error 2.2 Matrices and least-squares estimates 2.3 Regression inference 2.4 Geometric interpretation 2.5 In-sample prediction 2.6 Practical issues", " Chapter 2 Linear Regression To ease into machine learning, we start with regular linear models. There is one dependent variable, the \\(y\\), and \\(p\\) explanatory variables, the \\(x\\)’s. The data, or training sample, consists of \\(n\\) independent observations: \\[ (y_1,\\mathbf{x}_1), (y_2,\\mathbf{x}_2),\\ldots,(y_n,\\mathbf{x}_n). \\] For individual \\(i\\), \\(y_i\\) is the value of the one-dimensional dependent variable, and \\(\\mathbf{x}_i = (x_{i1}, x_{i2}, \\cdots, x_{ip})^t\\) is the \\(p\\times 1\\) vector of values for the explanatory variables. Generally, the \\(y_i\\)’s are continuous, but the \\(x_{ij}\\)’s can be anything numerical, e.g., 0-1 indicator variables, or functions of another variable (e.g., \\(x,x^2,x^3\\)). The linear model is \\[\\begin{equation} y_i = \\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_p x_{ip}+e_i. \\tag{2.1} \\end{equation}\\] The \\(\\beta_j\\)’s are parameters, usually unknown and to be estimated. The \\(e_i\\)’s are the errors or residuals. We will assume that The \\(e_i\\)’s are independent (of each other, and of the \\(\\mathbf{x}_i\\)’s); \\(\\mathbb{E} [e_i]=0\\) for each \\(i\\); \\(\\text{Var}[e_i] = \\sigma^2_e\\) for each \\(i\\). There is also a good chance we will assume they are normally distributed. From STAT424 and 425 (or other courses), you know what to do now: estimate the \\(\\beta_j\\)’s and \\(\\sigma^2_e\\), decide which \\(\\beta_j\\)’s are significant, do \\(F\\)-tests, look for outliers and other violations of the assumptions, etc. Here, we may do much of that, but with the goal of prediction. Suppose \\((y^*,\\mathbf{x}^*)\\) is a new point, satisfying the same model and assumptions as above (in particular, being independent of the observed \\(\\mathbf{x}_i\\)’s). Once we have the estimates of the \\(\\beta_j\\)’s (based on the observed data), we predict \\(y^*\\) from \\(\\mathbf{x}^*\\) by \\[\\begin{equation} \\widehat{y}^* = \\widehat{\\beta}_0+\\widehat\\beta_1 x^*_1+\\cdots+\\widehat\\beta_p x^{*}_p. \\tag{2.2} \\end{equation}\\] The prediction is good if \\(\\widehat{y}^{*}\\) is close to \\(y^{*}\\). We do not know \\(y^{*}\\), but we can hope. But the key point is The estimates of the parameters are good if they give good predictions. We don’t care if the \\(\\widehat\\beta_j\\)’s are close to the \\(\\beta_j\\)’s; we don’t care about unbiasedness or minimum variance or significance. We just care whether we get good predictions. 2.1 Good predictions: Squared error loss and in-sample error We want the predictions to be close to the actual (unobserved) value of the dependent variable, that is, we want \\(\\widehat{y}^{*}\\) close to \\(y^{*}\\). One way to measure closeness is by using squared error: \\[ (y^{*}-\\widehat{y}^{*})^2. \\] Because we do not know \\(y^{*}\\) (yet), we might look at the expected value instead: \\[ E[(Y^{*}-\\widehat{Y}^{*})^2]. \\] But what is that the expected value over? Certainly \\(Y^{*}\\), but the \\(Y_i\\)’s and \\(\\mathbf{X}_i\\)’s in the sample, as well as the \\(\\mathbf{X}^{*}\\), could all be considered random. There is no universal answer, but for our purposes here we will assume that the all features \\(\\mathbf{X}\\) are fixed, and all the \\(Y_i\\)’s are random, i.e., \\[\\begin{equation} E[(Y^{*}-\\widehat Y^{*})^2 \\mid \\mathbf{X}_1=\\mathbf{x}_1,\\ldots,\\mathbf{X}_n=\\mathbf{x}_n,\\mathbf{X}^{*}=\\mathbf{x}^{*}]. \\tag{2.3} \\end{equation}\\] But typically you are creating a predictor for many new \\(x\\)’s, and likely you do not know what they will be. (You don’t know what the next 1000 e-mails you get will be.) A reasonable approach is to assume the new \\(\\mathbf{x}\\)’s will look much like the old ones, hence you would look at the errors for \\(n\\) new \\(\\mathbf{x}_i\\)’s being the same as the old ones. Thus we would have \\(n\\) new cases, \\((y_i^{*},\\mathbf{x}_i^{*})\\), but where \\(\\mathbf{X}_i^{*}=\\mathbf{x}_i\\). The \\(n\\) expected errors are averaged, to obtain what is called the in-sample error: \\[\\begin{equation} \\text{ERR}_{\\text{in}} = \\frac{1}{n} \\sum_{i=1}^n E[(Y_i^{*}-\\widehat Y_i^{*})^2~|~\\mathbf{X}_1=\\mathbf{x}_1,\\ldots,\\mathbf{X}_n=\\mathbf{x}_n,\\mathbf{X}_i^{*}=\\mathbf{x}_i^{*}]. \\tag{2.4} \\end{equation}\\] In particular situations, you may have a more precise knowledge of what the new \\(x\\)’s would be. By all means, use those values. We will drop the conditional part of the notation for simplicity. 2.2 Matrices and least-squares estimates Ultimately we want to find estimates of the parameters that yield a low _{}. We’ll start with the least squares estimate, then translate things to matrices. The estimates of the \\(\\beta_j\\)’s depends on just the training sample. The least squares estimate of the parameters are the \\(\\beta_j\\)’s that minimize the objective function \\[\\begin{equation} \\text{RSS}(\\beta_0,\\ldots,\\beta_p) = \\sum_{i=1}^n (y_i-\\beta_0- \\beta_1 x_{i1}-\\cdots- \\beta_p x_{ip})^2. \\tag{2.5} \\end{equation}\\] The function is a nice convex function in the \\(\\beta_j\\)’s, so setting the derivatives equal to zero and solving will yield the minimum. The derivatives are \\[\\begin{align} \\frac{\\partial}{\\partial \\beta_0} \\text{RSS} (\\beta_0,\\ldots,\\beta_p) &amp;= -2~\\sum_{i=1}^n (y_i-\\beta_0-\\beta_1 x_{i1}-\\cdots-\\beta_p x_{ip});\\\\ \\frac{\\partial}{\\partial \\beta_j} \\text{RSS} (\\beta_0,\\ldots,\\beta_p) &amp;= -2~\\sum_{i=1}^n x_{ij}(y_i-\\beta_0-\\beta_1 x_{i1}-\\cdots-\\beta_p x_{ip}), \\quad j\\ge1. \\tag{2.6} \\end{align}\\] Write the equations in matrix form, starting with \\[\\begin{equation} \\begin{pmatrix} y_1-\\beta_0-\\beta_1 x_{11}-\\cdots-\\beta_p x_{1p}\\\\ y_2-\\beta_0-\\beta_1 x_{21}-\\cdots-\\beta_p x_{2p}\\\\ \\vdots\\\\ y_n-\\beta_0-\\beta_1 x_{n1}-\\cdots-\\beta_p x_{np} \\end{pmatrix} = \\begin{pmatrix} y_1\\\\y_2\\\\\\vdots\\\\y_n \\end{pmatrix} - \\begin{pmatrix} 1&amp;x_{11}&amp;x_{12}&amp;\\cdots&amp;x_{1p}\\\\ 1&amp;x_{21}&amp;x_{22}&amp;\\cdots&amp;x_{2p}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ 1&amp;x_{n1}&amp;x_{n2}&amp;\\cdots&amp;x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_p \\end{pmatrix} \\tag{2.7} \\end{equation}\\] which is equal to \\(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\). The \\(n\\)-by-\\((p+1)\\) matrix \\(\\mathbf{X}\\) is the so-called design matrix. Take the two summations in equations (2.6) (without the \\(-2\\)’s) and set to 0 to get \\[\\begin{align} \\begin{pmatrix}1&amp;1&amp;\\cdots&amp;1\\end{pmatrix}(\\mathbf{y}-\\mathbf{x}\\boldsymbol{\\beta})&amp;=0;\\\\ \\begin{pmatrix}x_{1j}&amp;x_{2j}&amp;\\cdots&amp;x_{nj}\\end{pmatrix}(\\mathbf{y}-\\mathbf{x}\\boldsymbol{\\beta})&amp;=0, \\quad j\\ge 1. \\tag{2.8} \\end{align}\\] Note that the row vectors in (2.8) on the left are the \\((p+1)\\) columns of \\(\\mathbf{X}\\), yielding \\[\\begin{equation} \\mathbf{X}^t(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}) = 0. \\tag{2.9} \\end{equation}\\] That equation is easily solved: \\[\\begin{equation} \\mathbf{X}^t\\mathbf{y} = \\mathbf{X}^t\\mathbf{x}\\boldsymbol{\\beta}~~\\Rightarrow~~\\boldsymbol{\\beta} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{y}, \\tag{2.10} \\end{equation}\\] at least if \\(\\mathbf{X}&#39;\\mathbf{X}\\) is invertible. If it is not invertible, then there will be many solutions. In practice, one can always eliminate some (appropriate) columns of \\(\\mathbf{X}\\) to obtain invertibility. Generalized inverses are available, too. Summary. In the linear model \\[\\begin{equation} \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e}, \\tag{2.11} \\end{equation}\\] where \\[\\begin{equation} \\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_p \\end{pmatrix} ~~\\text{and}~~ \\mathbf{e} = \\begin{pmatrix} e_1\\\\e_2\\\\\\vdots\\\\e_n \\end{pmatrix}, \\tag{2.12} \\end{equation}\\] the least squares estimate of \\(\\boldsymbol{\\beta}\\), assuming \\(\\mathbf{X}^t\\mathbf{X}\\) is invertible, is \\[\\begin{equation} \\widehat{\\boldsymbol{\\beta}}_{LS} = (\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{y}. \\tag{2.13} \\end{equation}\\] 2.3 Regression inference 2.4 Geometric interpretation 2.4.1 Basic concepts in vector spaces 2.4.2 LS and projection 2.5 In-sample prediction When considering the in-sample error for the linear model, we have the same model for the training sample and the new sample: \\[\\begin{equation} \\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}~~\\mbox{and}~~\\mathbf{Y}^{*} = \\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}^{*}. \\tag{2.14} \\end{equation}\\] The \\(e_i\\)’s and \\(e^{*}_i\\)’s are independent with mean 0 and variance \\(\\sigma^2_e\\). If we use the least-squares estimate of \\(\\boldsymbol{\\beta}\\) in the prediction, we have \\[\\begin{equation} \\widehat{\\mathbf{Y}}^{*} = \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}_{LS} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y} = \\mathbf{H}\\mathbf{Y}, \\tag{2.15} \\end{equation}\\] where \\(\\mathbf{H}\\) is the “hat” matrix, \\[\\begin{equation} \\mathbf{H} = \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;. \\tag{2.16} \\end{equation}\\] Note that this matrix is symmetric and idempotent, which means that \\[\\begin{equation} \\mathbf{H}^t = \\mathbf{H}, \\quad \\mathbf{H}\\mathbf{H}=\\mathbf{H}. \\tag{2.17} \\end{equation}\\] The errors in prediction are the \\(Y_i^{*} - \\widehat Y_i^{*}\\). Before getting to the \\(\\text{ERR}_{\\text{in}}\\), consider the mean and covariance’s of these errors. First, \\[\\begin{equation} E[\\mathbf{Y}] = E[\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}] = \\mathbf{X}\\boldsymbol{\\beta},~~E[\\mathbf{Y}^{*}] = E[\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}^{*}] = \\mathbf{X}\\boldsymbol{\\beta}, \\tag{2.18} \\end{equation}\\] because the expected values of the \\(e\\)’s are all 0 and we are assuming \\(\\mathbf{X}\\) is fixed, and \\[\\begin{align} E[\\widehat {\\mathbf{Y}}^{*}] &amp;= E[\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y}]\\\\ &amp;= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;E[\\mathbf{Y}]\\\\ &amp;= \\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{X}\\boldsymbol{\\beta}\\\\ &amp;= \\mathbf{X}\\boldsymbol{\\beta}, \\tag{2.19} \\end{align}\\] because the \\(\\mathbf{X}&#39;\\mathbf{X}\\)’s cancel. Thus, \\[\\begin{equation} \\mathbb{E}[\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}] = \\mathbf{0}_n~~\\mbox{(the $n\\times 1$ vector of 0&#39;s)}. \\tag{2.20} \\end{equation}\\] This zero means that the errors are unbiased. They may be big or small, but on average right on the nose. Unbiasedness is ok, but it is really more important to be close. Next, the covariance matrices: \\[\\begin{equation} \\text{Cov}[\\mathbf{Y}] = \\text{Cov}[\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{e}] = \\text{Cov}[\\mathbf{e}] = \\sigma_e^2\\mathbf{I}_n~~\\mbox{(the $n\\times n$ identity matrix)}, \\tag{2.21} \\end{equation}\\] because the \\(e_i\\)’s are independent, hence have zero covariance, and all have variance \\(\\sigma^2_e\\). Similarly, \\[\\begin{equation} \\text{Cov}[\\mathbf{Y}^{*}] = \\sigma_e^2\\mathbf{I}_n. \\tag{2.22} \\end{equation}\\] Less similar, \\[\\begin{align} \\text{Cov}[\\widehat {\\mathbf{Y}}^{*}] &amp;= \\text{Cov}[\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{Y}]\\\\ &amp;=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\text{Cov}[\\mathbf{Y}] \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t \\sigma^2_e\\mathbf{I}_n \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\sigma^2_e\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\sigma^2_e\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\\\ &amp;=\\sigma^2_e\\mathbf{H}, \\tag{2.23} \\end{align}\\] the second line following from (2.16). Finally, for the errors, note that \\(\\mathbf{Y}^{*}\\) and \\(\\widehat{\\mathbf{Y}}^{*}\\) are independent, because the latter depends on the training sample alone. Hence, \\[\\begin{align} \\text{Cov}[\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}] &amp;= \\text{Cov}[\\mathbf{Y}^{*}] +Cov[\\widehat {\\mathbf{Y}}^{*}] ~~\\mbox{(notice the $+$)}\\\\ &amp;=\\sigma^2_e\\mathbf{I}_n+\\sigma^2_e\\mathbf{H}\\\\ &amp;=\\sigma^2_e(\\mathbf{I}_n+\\mathbf{H}). \\tag{2.24} \\end{align}\\] Now, \\[\\begin{align} n \\cdot \\text{ERR}_{\\text{in}} &amp;= \\mathbb{E} [\\|\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}\\|^2]\\\\ &amp;=\\|\\mathbb{E} \\mathbf{Y}^{*} - \\mathbb{E} \\widehat {\\mathbf{Y}}^{*} \\|^2+ \\text{tr}(\\text{Cov}[\\mathbf{Y}^{*} - \\widehat {\\mathbf{Y}}^{*}])\\\\ &amp;=\\text{tr} (\\sigma^2_e(\\mathbf{I}_n+\\mathbf{H}))\\\\ &amp;=\\sigma^2_e(n+ \\text{tr}(\\mathbf{H})). \\tag{2.25} \\end{align}\\] The third line follows from (2.24) and (2.20), and the second from the following result: for any random vector \\(\\mathbf{Z}\\) \\[\\begin{align} \\mathbb{E} [\\|\\mathbf{Z} \\|^2] &amp;= \\mathbb{E} [Z_1^2+\\cdots+Z_m^2]\\\\ &amp;= \\mathbb{E}[Z_1^2]+\\cdots+\\mathbb{E}[Z_m^2] \\\\ &amp;= \\mathbb{E}[Z_1]^2+\\text{Var}[Z_1]^2+\\cdots+\\mathbb{E} [Z_m]^2+ \\text{Var}[Z_m]^2\\\\ &amp;= \\|\\mathbb{E} \\mathbf{Z}\\|^2 + \\text{tr}(\\text{Cov}[\\mathbf{Z}]), \\tag{2.26} \\end{align}\\] where the trace of a matrix is the sum of the diagonals, which in the case of a covariance matrix are the variances. For the trace, recall that \\(\\mathbf{X}\\) is \\(n\\times(p+1)\\), so that \\[\\begin{align} \\text{tr}(\\mathbf{H}) &amp;= \\text{tr}(\\mathbf{X}(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;) \\\\ &amp;= \\text{tr}((\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{X}) \\\\ &amp;= \\text{tr}(\\mathbf{I}_{p+1})=p+1. \\tag{2.27} \\end{align}\\] Putting that answer in (2.25) we obtain \\[\\begin{equation} \\text{ERR}_{\\text{in}} = \\sigma^2_e + \\sigma^2_e \\frac{p+1}{n}. \\tag{2.28} \\end{equation}\\] This expected in-sample error is a simple function of three quantities. We will use it as a benchmark. The goal in the rest of this section will be to find, if possible, predictors that have lower in-sample error. There’s not much we can do about \\(\\sigma^2_e\\), since it is the inherent variance of the observations. Taking a bigger training sample will decrease the error, as one would expect. The one part we can work with is the \\(p\\), that is, try to reduce \\(p\\) by eliminating some of the explanatory variables. Will that strategy work? It is the subject of the next chapter. 2.6 Practical issues 2.6.1 Using R 2.6.2 Interpret LS coefficients 2.6.3 Handle categorical variables 2.6.4 Outliers and rank deficiency "],["variable-selection-and-regularization.html", "Chapter 3 Variable Selection and Regularization 3.1 Subset selection 3.2 Ridge regression 3.3 Lasso regression", " Chapter 3 Variable Selection and Regularization Notes [lec_W3_VariableSelection.pdf] [lec_W3_PCR.pdf] [lec_W3_More_on_Ridge_Lasso.pdf] R/Python Code [Rcode_W3_VarSel_SubsetSelection] [Rcode_W3_VarSel_RidgeLasso] 3.1 Subset selection 3.2 Ridge regression 3.3 Lasso regression "],["regression-trees.html", "Chapter 4 Regression Trees", " Chapter 4 Regression Trees "],["nonlinear-regression.html", "Chapter 5 Nonlinear Regression 5.1 Polynomials 5.2 Regression Splines 5.3 Smoothing Splines 5.4 Sinces and cosines 5.5 A glimpse of wavelets", " Chapter 5 Nonlinear Regression Chapter 2 assumed that the mean of the dependent variables was a linear function of the explanatory variables. In this chapter we will consider non-linear functions. We start with just one \\(x\\)-variable, and consider the model \\[\\begin{equation} Y_i = f(x_i)+e_i,~~i=1,\\ldots,n, \\tag{5.1} \\end{equation}\\] where the \\(x_i\\)’s are given, and the \\(e_i\\)’s are independent with mean zero and variances \\(\\sigma^2_e\\). A linear model would have \\(f(x_i)=\\beta_0+\\beta_1x_i\\). Here, we are not constraining \\(f\\) to be linear, or even any parametric function. Basically, \\(f\\) can be any function as long as it is sufficiently smooth. Exactly what we mean by smooth will be detailed later. From a prediction point of view, the goal is to find an estimated function of \\(f\\), \\(\\widehat f\\), so that new \\(y\\)’s can be predicted from new \\(x\\)’s by \\(\\widehat f(x)\\). Related but not identical goals include Curve-fitting: fit a smooth curve to the data in order to have a good summary of the data; find a curve so that the graph “looks nice”; Interpolation: Estimate \\(y\\) for values of \\(x\\) not among the observed, but in the same range as the observed; Extrapolation: Estimate \\(y\\) for values of \\(x\\) outside the range of the observed \\(x\\)’s, a somewhat dangerous activity. This chapter deals with nonparametric functions \\(f\\), which strictly speaking means that we are not assuming a particular form of the function based on a finite number of parameters. Examples of parametric nonlinear functions: \\[\\begin{equation} f(x) = \\alpha~e^{\\beta x}~~\\text{and}~~f(x) =\\sin(\\alpha+\\beta~x+\\gamma x^2). \\tag{5.2} \\end{equation}\\] Such models can be fit with least squares much as the linear models, although the derivatives are not simple linear functions of the parameters, and Newton-Raphson or some other numerical method is needed. The approach we take to estimating \\(f\\) in the nonparametric model is to use some sort of basis expansion of the functions on \\(\\mathbb{R}\\). That is, we have an infinite set of known functions, \\(h_1(x), h_2(x),\\ldots,\\) and estimate \\(f\\) using a linear combination of a subset of the functions, e.g., \\[\\begin{equation} \\widehat f(x) = \\widehat{\\beta}_0+\\widehat{\\beta}_1 h_1(x) +\\cdots+\\widehat{\\beta}_m h_m(x). \\tag{5.3} \\end{equation}\\] We are not assuming that \\(f\\) is a finite linear combination of the \\(h_j\\)’s, hence will always have a biased estimator of \\(f\\). Usually we do assume that \\(f\\) can be arbitrarily well approximated by such a linear combination, that is, there is a sequence \\(\\beta_0,\\beta_1,\\beta_2,\\ldots,\\) such that \\[\\begin{equation} f(x) = \\beta_0+\\lim_{m\\to\\infty}\\sum_{i=1}^m \\beta_j h_j(x) \\tag{5.4} \\end{equation}\\] uniformly, at least for \\(x\\) in a finite range. An advantage to using estimates as in (5.3) is that the estimated function is a linear one, linear in the \\(h_j\\)’s, though not in the \\(x\\). But with \\(x_i\\)’s fixed, we are in the same estimation bailiwick as the linear models in Chapter 2, hence ideas such as subset selection, ridge, lasso, and estimating prediction errors carry over reasonably easily. In the next few sections, we consider possible sets of \\(h_j\\)’s, including polynomials and local polynomials such as cubic splines. 5.1 Polynomials The estimate of \\(f\\) is a polynomial in \\(x\\), where the challenge is to figure out the degree. In raw form, we have \\[\\begin{equation} h_1(x) = x,~h_2(x) = x^2,~h_3(x) = x^3, \\ldots. \\tag{5.5} \\end{equation}\\] (The Weierstrass Approximation Theorem guarantees that (5.4) holds.) The \\(m^{th}\\) degree polynomial fit is then \\[\\begin{equation} \\widehat f(x) = \\widehat\\beta_0+ \\widehat\\beta_1x+\\widehat\\beta_2x^2+\\cdots+\\widehat\\beta_mx^m. \\tag{5.6} \\end{equation}\\] It is straightforward to find the estimates \\(\\widehat\\beta_j\\) using the techniques from Chapter 2, where here \\[\\begin{equation} \\mathbf{X} = \\begin{pmatrix} 1&amp;x_1&amp;x_1^2&amp;\\cdots&amp;x_1^m\\\\ 1&amp;x_2&amp;x_2^2&amp;\\cdots&amp;x_2^m\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ 1&amp;x_N&amp;x_N^2&amp;\\cdots&amp;x_N^m \\end{pmatrix}. \\tag{5.7} \\end{equation}\\] Technically, one could perform a regular subset regression procedure, but generally one considers only fits allowing the first \\(m\\) coefficients to be nonzero, and requiring the rest to be zero. Thus the only fits considered are \\[\\begin{equation} \\begin{array}{rclc} \\widehat f(x_i) &amp;=&amp; \\widehat{\\beta}_0&amp;\\text{(constant)}\\\\ \\widehat f(x_i) &amp;=&amp; \\widehat\\beta_0+ \\widehat\\beta_1x_i&amp;\\text{(linear)}\\\\ \\widehat f(x_i) &amp;=&amp; \\widehat\\beta_0+ \\widehat\\beta_1x_i+ \\widehat\\beta_2x_i^2&amp;\\text{(quadratic)}\\\\ &amp;\\vdots&amp;\\\\ \\widehat f(x_i) &amp;=&amp; \\widehat\\beta_0+ \\widehat\\beta_1x_i+\\widehat\\beta_2x_i^2+\\cdots+\\widehat\\beta_mx_i^m &amp;\\text{($m^{th}$ degree)}\\\\ &amp;\\vdots&amp; \\end{array} \\tag{5.8} \\end{equation}\\] We will use the birthrate data to illustrate polynomial fits. The \\(x\\)’s are the years from 1917 to 2003, and the \\(y\\)’s are the births per 10,000 women aged twenty-three in the U.S.1 Figure ?? contains the fits of several polynomials, from a cubic to an \\(80^{th}\\)-degree polynomial. It looks like the \\(m=3\\) and \\(m=5\\) fits are poor, \\(m=20\\) to 40 are reasonable, and \\(m=80\\) is overfitting, i.e., the curve is too jagged. 5.1.1 Model selection 5.1.2 Using R The \\(\\mathbf{X}\\) matrix in (5.7) is not the one to use for computations. For any high-degree polynomial, we will end up with huge numbers (e.g., \\(87^{16}\\approx 10^{31}\\)) and small numbers in the matrix, which leads to numerical inaccuracies. A better matrix uses orthogonal polynomials, or in fact orthonormal ones. Thus we want the columns of the \\(\\mathbf{X}\\) matrix, except for the intercept column \\(\\mathbf{1}_n\\), to have mean 0 and norm 1, but also to have them orthogonal to each other in such a way that the first \\(m\\) columns still yield the \\(m^{th}\\)-degree polynomials. To illustrate, without going into much detail, we use the Gram-Schmidt algorithm for \\(\\mathbf{x}=(1,2,3,4,5)&#39;\\) and \\(m=2\\). Start with the raw columns, \\[\\begin{equation} \\mathbf{1}_5=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\\\1\\end{pmatrix}, \\mathbf{x}_{[1]} = \\begin{pmatrix}1\\\\2\\\\3\\\\4\\\\5\\end{pmatrix},~~\\text{and}~~ \\mathbf{x}_{[2]} = \\begin{pmatrix}1\\\\4\\\\9\\\\16\\\\25\\end{pmatrix}. \\tag{5.9} \\end{equation}\\] Let the first one as is, but subtract the means (3 and 11) from each of the other two: \\[\\begin{equation} {\\mathbf{x}}^{(2)}_{[1]} = \\begin{pmatrix}-2\\\\-1\\\\0\\\\1\\\\2\\end{pmatrix},~~\\text{and}~~ {\\mathbf{x}}^{(2)}_{[2]} = \\begin{pmatrix}-10\\\\-7\\\\-2\\\\5\\\\14\\end{pmatrix}. \\tag{5.10} \\end{equation}\\] Now leave the first two alone, and make the third orthogonal to the second by applying the main Gram-Schmidt step, \\[\\begin{equation} \\mathbf{u} \\to \\mathbf{u} - \\frac{\\mathbf{u}^t \\mathbf{v}}{\\mathbf{v}^t\\mathbf{v}} \\cdot \\mathbf{v}, \\tag{5.11} \\end{equation}\\] with \\(\\mathbf{v}={\\mathbf{x}}^{(2)}_{[1]}\\) and \\(\\mathbf{u}={\\mathbf{x}}^{(2)}_{[2]}\\): \\[\\begin{equation} \\mathbf{x}_{[2]}^{[3]} = \\begin{pmatrix}-10\\\\-7\\\\-2\\\\5\\\\14\\end{pmatrix} - \\frac{60}{10}~\\begin{pmatrix}-2\\\\-1\\\\0\\\\1\\\\2\\end{pmatrix} = \\begin{pmatrix} 2\\\\ -1\\\\ -2\\\\ -1\\\\ 2\\end{pmatrix}. \\tag{5.12} \\end{equation}\\] To complete the picture, divide the last two \\(x\\)’s by their respective norms, to get \\[\\begin{equation} \\mathbf{1}_5=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\\\1\\end{pmatrix}, \\mathbf{x}^{Norm}_{[1]} = \\frac{1}{\\sqrt{10}}~\\begin{pmatrix}-2\\\\-1\\\\0\\\\1\\\\2\\end{pmatrix},~~\\text{and}~~ {\\mathbf{x}}^{Norm}_{[2]} = \\frac{1}{\\sqrt{14}} \\begin{pmatrix}-10\\\\-7\\\\-2\\\\5\\\\14\\end{pmatrix}. \\tag{5.13} \\end{equation}\\] You can check that indeed these three vectors are orthogonal, and the last two orthonormal. For a large \\(n\\) and \\(m\\), you would continue, at step \\(k\\) orthogonalizing the current \\((k+1)^{st},\\ldots,(m+1)^{st}\\) vector to the current \\(k^{th}\\) vector. Once you have these vectors, then the fitting is easy, because the \\(\\mathbf{X}_m\\) for the \\(m^{th}\\) degree polynomial (leaving out the \\(\\mathbf{1}_n\\)) just uses the first \\(m\\) vectors, and \\(\\mathbf{X}_m^t\\mathbf{X}_m=\\mathbf{I}_m\\), so that the estimates of beta are just \\(\\mathbf{X}^t_m\\mathbf{y}\\), and the \\(\\mathbf{H}_m=\\mathbf{X}_m\\mathbf{X}_m^t\\). Using the saturated model, i.e., \\((n-1)^{st}\\)-degree, we can get all the coefficients at once, \\[\\begin{equation} \\widehat{\\boldsymbol{\\beta}} = \\mathbf{X}_{n-1}^t\\mathbf{y},~~(\\widehat\\beta_0=\\overline y). \\tag{5.14} \\end{equation}\\] Then the coefficients for the \\(m^{th}\\) order fit are the first \\(m\\) elements of \\(\\widehat{\\boldsymbol{\\beta}}\\). Also, the residual sum of squares equals the sum of squares of the left-out coefficients: \\[\\begin{equation} \\text{RSS}_m = \\sum_{j=m+1}^{n-1} \\widehat\\beta_j^2, \\tag{5.15} \\end{equation}\\] from which it is easy to find the residual variances, \\(\\overline {err}^m\\)’s, and estimated prediction errors. Unfortunately, polynomials are not very good for extrapolation. Using the two polynomial fits, we have the following extrapolations. 5.2 Regression Splines 5.3 Smoothing Splines 5.4 Sinces and cosines 5.5 A glimpse of wavelets The data up through 1975 can be found in the Data and Story Library at http://lib.stat.cmu.edu/DASL/Datafiles/Birthrates.html. See Velleman, P. F. and Hoaglin, D. C. (1981). Applications, Basics, and Computing of Exploratory Data Analysis. Belmont. CA: Wadsworth, Inc. The original data is from P.K. Whelpton and A. A. Campbell, “Fertility Tables for Birth Charts of American Women,” Vital Statistics Special Reports 51, no. 1. (Washington D.C.:Government Printing Office, 1960, years 1917-1957) and National Center for Health Statistics, Vital Statistics of the United States Vol. 1, Natality (Washington D.C.:Government Printing Office, yearly, 1958-1975). The data from 1976 to 2003 are actually rates for women aged 20-24, found in the National Vital Statistics Reports Volume 54, Number 2 September 8, 2005, Births: Final Data for 2003; http://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54\\_02.pdf.↩︎ "],["clustering-analysis.html", "Chapter 6 Clustering Analysis", " Chapter 6 Clustering Analysis "],["latent-structure-models.html", "Chapter 7 Latent Structure Models", " Chapter 7 Latent Structure Models "],["more-on-clustering.html", "Chapter 8 More on Clustering", " Chapter 8 More on Clustering "],["distriminant-analysis.html", "Chapter 9 Distriminant Analysis", " Chapter 9 Distriminant Analysis "],["logistic-regression.html", "Chapter 10 Logistic Regression", " Chapter 10 Logistic Regression "],["support-vector-machines.html", "Chapter 11 Support Vector Machines", " Chapter 11 Support Vector Machines "],["classification-trees-and-boosting.html", "Chapter 12 Classification Trees and Boosting", " Chapter 12 Classification Trees and Boosting "],["recommender-system.html", "Chapter 13 Recommender System", " Chapter 13 Recommender System "],["introduction-to-deep-learning.html", "Chapter 14 Introduction to Deep Learning", " Chapter 14 Introduction to Deep Learning "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
